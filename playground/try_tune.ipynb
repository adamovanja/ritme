{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try out raytune with scikit & keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from q2_time import _static_searchspace as ss\n",
    "from q2_time import _static_trainables as st\n",
    "from q2_time.config import HOST_ID, MLFLOW_TRACKING_URI, SEED_DATA, SEED_MODEL, TARGET\n",
    "from q2_time.process_data import load_n_split_data\n",
    "from q2_time.tune_models import run_trials\n",
    "from tensorflow.keras.models import load_model\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, test = load_n_split_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_xgb = run_trials(\n",
    "    MLFLOW_TRACKING_URI,\n",
    "    \"xgb\",\n",
    "    st.train_xgb,\n",
    "    ss.xgb_space,\n",
    "    train_val,\n",
    "    TARGET,\n",
    "    HOST_ID,\n",
    "    SEED_DATA,\n",
    "    SEED_MODEL,\n",
    "    fully_reproducible=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nn = run_trials(\n",
    "    MLFLOW_TRACKING_URI,\n",
    "    \"nn\",\n",
    "    st.train_nn,\n",
    "    ss.nn_space,\n",
    "    train_val,\n",
    "    TARGET,\n",
    "    HOST_ID,\n",
    "    SEED_DATA,\n",
    "    SEED_MODEL,\n",
    "    fully_reproducible=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_linreg = run_trials(\n",
    "    MLFLOW_TRACKING_URI,\n",
    "    \"linreg\",\n",
    "    st.train_linreg,\n",
    "    ss.linreg_space,\n",
    "    train_val,\n",
    "    TARGET,\n",
    "    HOST_ID,\n",
    "    SEED_DATA,\n",
    "    SEED_MODEL,\n",
    "    fully_reproducible=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf = run_trials(\n",
    "    MLFLOW_TRACKING_URI,\n",
    "    \"rf\",\n",
    "    st.train_rf,\n",
    "    ss.rf_space,\n",
    "    train_val,\n",
    "    TARGET,\n",
    "    HOST_ID,\n",
    "    SEED_DATA,\n",
    "    SEED_MODEL,\n",
    "    fully_reproducible=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter for getting best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras case: works with changes in trainable stored\n",
    "def load_best_checkpoint(checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "    # todo: add different loading based on framework used in training\n",
    "    best_model = load_model(checkpoint_path)\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = results_nn.get_best_result()\n",
    "\n",
    "# Load the best model\n",
    "best_checkpoint_dir = best_trial.checkpoint.to_directory()\n",
    "best_model = load_best_checkpoint(best_checkpoint_dir)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb case\n",
    "best_trial = results_xgb.get_best_result()\n",
    "\n",
    "# Load the best model\n",
    "best_checkpoint_dir = best_trial.checkpoint.to_directory()\n",
    "best_checkpoint_path = os.path.join(best_checkpoint_dir, \"checkpoint\")\n",
    "best_model = xgb.Booster(model_file=best_checkpoint_path)\n",
    "best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_linreg.get_best_result().config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(exp_name, trial_result, metric_ls=[\"rmse_train\", \"rmse_val\"]):\n",
    "    # Get the result with the metric and mode defined in tune_config before\n",
    "    best_result = trial_result.get_best_result()\n",
    "    config = best_result.config\n",
    "\n",
    "    # # get config of best model\n",
    "    # best_result.config\n",
    "    # # todo: find out how to extract the best performing model (pkl)\n",
    "    # best_result.best_checkpoints\n",
    "\n",
    "    metrics_ser = best_result.metrics_dataframe[metric_ls].iloc[-1]\n",
    "    metrics_df = pd.DataFrame({exp_name: metrics_ser})\n",
    "    return metrics_df, config\n",
    "\n",
    "\n",
    "def calc_best_metrics(dic_trials):\n",
    "    df_metrics = pd.DataFrame(index=[\"rmse_train\", \"rmse_val\"])\n",
    "    dic_config = {}\n",
    "    for key, value in dic_trials.items():\n",
    "        df_best, config = get_best_model(key, value)\n",
    "        df_metrics = df_metrics.join(df_best)\n",
    "        dic_config[key] = config\n",
    "\n",
    "    return df_metrics, pd.DataFrame(dic_config)\n",
    "\n",
    "\n",
    "def plot_best_metrics(df_metrics):\n",
    "    df2plot = df_metrics.T.sort_values(\"rmse_val\", ascending=True)\n",
    "    df2plot.columns = [\"train\", \"validation\"]\n",
    "    # plot settings\n",
    "    # todo: set default plot settings across package\n",
    "    plt.style.use(\"seaborn-v0_8-colorblind\")  # (\"tableau-colorblind10\")\n",
    "    titlesize = 14\n",
    "    labelsize = 13\n",
    "    ticklabel = 12\n",
    "    plt.rcParams.update({\"font.size\": labelsize})\n",
    "\n",
    "    df2plot.plot(kind=\"bar\", figsize=(12, 6))\n",
    "\n",
    "    plt.xticks(fontsize=ticklabel)\n",
    "    plt.yticks(fontsize=ticklabel)\n",
    "    plt.ylabel(\"RMSE\", fontsize=labelsize)\n",
    "    plt.xlabel(\"Model type (order: increasing val score)\", fontsize=labelsize)\n",
    "    plt.title(\"Metrics comparison\", fontsize=titlesize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all, best_configs = calc_best_metrics(\n",
    "    {\"xgb\": results_xgb, \"linreg\": results_linreg, \"nn\": results_nn, \"rf\": results_rf}\n",
    ")\n",
    "plot_best_metrics(metrics_all)\n",
    "display(best_configs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate over training time (example for xgb model here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the result with the maximum test set `mean_accuracy`\n",
    "best_xgb = results_xgb.get_best_result()\n",
    "best_xgb.metrics_dataframe.plot(\"training_iteration\", [\"rmse_train\", \"rmse_val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = None\n",
    "for result in results_xgb:\n",
    "    label = f\"data_transform={result.config['data_transform']}, \\\n",
    "        max_depth={result.config['max_depth']}\"\n",
    "    if ax is None:\n",
    "        ax = result.metrics_dataframe.plot(\n",
    "            \"training_iteration\", \"rmse_val\", label=label\n",
    "        )\n",
    "    else:\n",
    "        result.metrics_dataframe.plot(\n",
    "            \"training_iteration\", \"rmse_val\", ax=ax, label=label\n",
    "        )\n",
    "ax.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "\n",
    "ax.set_title(\"rsme_val vs. training iteration for all trials\")\n",
    "ax.set_ylabel(\"RMSE_val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raytune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
