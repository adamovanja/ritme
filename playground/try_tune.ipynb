{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try out raytune with scikit & keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from ray import air, tune\n",
    "from ray.air import session\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.tune.integration.keras import TuneReportCallback\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler, HyperBandScheduler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "from q2_time.engineer_features import transform_features\n",
    "from q2_time.model import split_data_by_host\n",
    "from q2_time.simulate_data import simulate_data\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a simulated dataset\n",
    "host_id = \"host_id\"\n",
    "target = \"age_days\"\n",
    "train_size = 0.8\n",
    "seed_data = 12\n",
    "seed_model = 12\n",
    "\n",
    "# todo: potentially rename columns to track md and ft columns\n",
    "ft, md = simulate_data(100)\n",
    "data = md.join(ft, how=\"left\")\n",
    "data.sort_values([host_id, target], inplace=True)\n",
    "\n",
    "# (train+val) & test split\n",
    "train_val, test = split_data_by_host(data, host_id, train_size, seed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(config, train_val, target, seed_data):\n",
    "    non_features_ls = [target, \"host_id\"]\n",
    "    # todo: redefine features selection here -> config\n",
    "    features_ls = [x for x in train_val.columns if x not in non_features_ls]\n",
    "\n",
    "    # feature engineering method -> config\n",
    "    if config[\"data_transform\"] is None:\n",
    "        ft_transformed = train_val[features_ls].copy()\n",
    "    else:\n",
    "        ft_transformed = transform_features(\n",
    "            train_val[features_ls], config[\"data_transform\"], config[\"alr_denom_idx\"]\n",
    "        )\n",
    "    train_val_t = train_val[non_features_ls].join(ft_transformed)\n",
    "\n",
    "    # train & val split - for training purposes\n",
    "    train, val = split_data_by_host(train_val_t, host_id, 0.8, seed_data)\n",
    "    X_train, y_train = train[ft_transformed.columns], train[target]\n",
    "    X_val, y_val = val[ft_transformed.columns], val[target]\n",
    "    return X_train.values, y_train.values, X_val.values, y_val.values\n",
    "\n",
    "\n",
    "def predict_score(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! define model training functions\n",
    "\n",
    "\n",
    "# Linear Regression (for consistency with other training)\n",
    "def train_linreg(config, train_val, target, seed_data, seed_model):\n",
    "    # ! process dataset\n",
    "    X_train, y_train, X_val, y_val = process_data(config, train_val, target, seed_data)\n",
    "\n",
    "    # ! model\n",
    "    np.random.seed(seed_model)\n",
    "    linreg = LinearRegression(fit_intercept=config[\"fit_intercept\"])\n",
    "    linreg.fit(X_train, y_train)\n",
    "\n",
    "    score_train = predict_score(linreg, X_train, y_train)\n",
    "    score_val = predict_score(linreg, X_val, y_val)\n",
    "    session.report({\"mse_val\": score_val, \"mse_train\": score_train})\n",
    "\n",
    "\n",
    "# Define a training function for RandomForest\n",
    "def train_rf(config, train_val, target, seed_data, seed_model):\n",
    "    # ! process dataset\n",
    "    X_train, y_train, X_val, y_val = process_data(config, train_val, target, seed_data)\n",
    "\n",
    "    # ! model\n",
    "    # setting seed for scikit library\n",
    "    np.random.seed(seed_model)\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=config[\"n_estimators\"], max_depth=config[\"max_depth\"]\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    score_train = predict_score(rf, X_train, y_train)\n",
    "    score_val = predict_score(rf, X_val, y_val)\n",
    "\n",
    "    session.report({\"mse_val\": score_val, \"mse_train\": score_train})\n",
    "\n",
    "\n",
    "# Define a training function for Keras neural network\n",
    "def train_nn(config, train_val, target, seed_data, seed_model):\n",
    "    # ! process dataset\n",
    "    X_train, y_train, X_val, y_val = process_data(config, train_val, target, seed_data)\n",
    "\n",
    "    # ! model\n",
    "    # set seeds\n",
    "    random.seed(seed_model)\n",
    "    np.random.seed(seed_model)\n",
    "    tf.random.set_seed(seed_model)\n",
    "    tf.compat.v1.set_random_seed(seed_model)\n",
    "\n",
    "    # define neural network\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "    n_layers = config[\"n_layers\"]\n",
    "    for i in range(n_layers):\n",
    "        num_hidden = config[f\"n_units_l{i}\"]\n",
    "        model.add(layers.Dense(num_hidden, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    # define learning\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
    "\n",
    "    # todo: reconsider adding early stopping\n",
    "    # early_stopping = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    mlflow.tensorflow.autolog()\n",
    "\n",
    "    # Add TuneReportCallback to report metrics for each epoch\n",
    "    report_callback = TuneReportCallback(\n",
    "        {\"mse_val\": \"val_mse\", \"mse_train\": \"mse\"}, on=\"epoch_end\"\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        callbacks=[report_callback],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # score_train = predict_score(model, X_train, y_train)\n",
    "    # score_val = predict_score(model, X_val, y_val)\n",
    "\n",
    "    # session.report({\"mse_val\": score_val, \"mse_train\": score_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! define hyperparameter search spaces\n",
    "# ! data engineering\n",
    "data_eng_space = {\n",
    "    # adjust to tune.grid_search checks all options:\n",
    "    # new nb_trials= num_trials * nb of options in data_transform\n",
    "    \"data_transform\": tune.grid_search([None, \"clr\", \"ilr\", \"alr\"]),\n",
    "    # todo: remove manual setting of max number of features (19 now)\n",
    "    \"alr_denom_idx\": tune.randint(0, 19),\n",
    "}\n",
    "\n",
    "# ! adding models\n",
    "# linear regression\n",
    "linreg_space = {\n",
    "    **data_eng_space,\n",
    "    \"fit_intercept\": tune.choice([True]),\n",
    "}\n",
    "\n",
    "# scikit random forest\n",
    "rf_space = {\n",
    "    **data_eng_space,\n",
    "    \"n_estimators\": tune.randint(100, 1000),\n",
    "    \"max_depth\": tune.randint(2, 32),\n",
    "    \"min_samples_split\": tune.choice([0.0001, 0.001, 0.01, 0.1]),\n",
    "    \"min_samples_leaf\": tune.choice([0.00001, 0.0001, 0.001]),\n",
    "    \"max_features\": tune.choice([None, \"sqrt\", \"log2\", 0.1, 0.2, 0.5, 0.8]),\n",
    "    \"min_impurity_decrease\": tune.choice([0.0001, 0.001, 0.01]),\n",
    "    \"bootstrap\": tune.choice([True, False]),\n",
    "}\n",
    "\n",
    "# keras neural network\n",
    "nn_space = {\n",
    "    **data_eng_space,\n",
    "    # Sample random uniformly between [1,9] rounding to multiples of 3\n",
    "    \"n_layers\": tune.qrandint(1, 9, 3),\n",
    "    \"learning_rate\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"batch_size\": tune.choice([32, 64, 128]),\n",
    "}\n",
    "for i in range(9):\n",
    "    nn_space[f\"n_units_l{i}\"] = tune.randint(3, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials(\n",
    "    mlflow_tracking_uri,  # MLflow with MLflowLoggerCallback\n",
    "    exp_name,\n",
    "    trainable,\n",
    "    search_space,\n",
    "    train_val,\n",
    "    target,\n",
    "    seed_data,\n",
    "    seed_model,\n",
    "    fully_reproducible=False,  # if true hyperband instead of ASHA scheduler is used\n",
    "    num_trials=2,  # todo: increase default num_trials\n",
    "    scheduler_grace_period=5,\n",
    "    scheduler_max_t=100,\n",
    "    resources={\"cpu\": 1},\n",
    "):\n",
    "    # set seed for search algorithms/schedulers\n",
    "    random.seed(seed_model)\n",
    "    np.random.seed(seed_model)\n",
    "    tf.random.set_seed(seed_model)\n",
    "\n",
    "    if not fully_reproducible:\n",
    "        # AsyncHyperBand enables aggressive early stopping of bad trials.\n",
    "        # ! efficient & fast BUT\n",
    "        # ! not fully reproducible with seeds (caused by system load, network\n",
    "        # ! communication and other factors in env) due to asynchronous mode only\n",
    "        scheduler = AsyncHyperBandScheduler(\n",
    "            # stop trials at least this old in time (measured in training iteration)\n",
    "            grace_period=scheduler_grace_period,\n",
    "            # stopping trials after max_t iterations have passed\n",
    "            max_t=scheduler_max_t,\n",
    "        )\n",
    "    else:\n",
    "        # ! slower BUT\n",
    "        # ! improves the reproducibility of experiments by ensuring that all trials\n",
    "        # ! are evaluated in the same order.\n",
    "        scheduler = HyperBandScheduler(max_t=scheduler_max_t)\n",
    "\n",
    "    analysis_rf = tune.Tuner(\n",
    "        # trainable with input parameters passed and set resources\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(\n",
    "                trainable,\n",
    "                train_val=train_val,\n",
    "                target=target,\n",
    "                seed_data=seed_data,\n",
    "                seed_model=seed_model,\n",
    "            ),\n",
    "            resources,\n",
    "        ),\n",
    "        # mlflow\n",
    "        run_config=air.RunConfig(\n",
    "            # experiment name: with subfolders with trials within\n",
    "            name=\"mlflow\",\n",
    "            callbacks=[\n",
    "                MLflowLoggerCallback(\n",
    "                    tracking_uri=mlflow_tracking_uri,\n",
    "                    experiment_name=exp_name,\n",
    "                    save_artifact=True,\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "        # hyperparameter space\n",
    "        param_space=search_space,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            # todo: consider taking RMSE loss\n",
    "            metric=\"mse_val\",\n",
    "            mode=\"min\",\n",
    "            # define the scheduler\n",
    "            scheduler=scheduler,\n",
    "            # number of trials to run\n",
    "            num_samples=num_trials,\n",
    "            # ! set seed\n",
    "            search_alg=tune.search.BasicVariantGenerator(),\n",
    "        ),\n",
    "    )\n",
    "    return analysis_rf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_tracking_uri = \"mlruns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf = run_trials(\n",
    "    mlflow_tracking_uri,\n",
    "    \"rf\",\n",
    "    train_rf,\n",
    "    rf_space,\n",
    "    train_val,\n",
    "    target,\n",
    "    seed_data,\n",
    "    seed_model,\n",
    "    fully_reproducible=False,\n",
    ")\n",
    "print(\"Best hyperparameters found were: \", results_rf.get_best_result().config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first test\n",
    "results_nn = run_trials(\n",
    "    mlflow_tracking_uri,\n",
    "    \"nn\",\n",
    "    train_nn,\n",
    "    nn_space,\n",
    "    train_val,\n",
    "    target,\n",
    "    seed_data,\n",
    "    seed_model,\n",
    "    fully_reproducible=False,\n",
    ")\n",
    "print(\"Best hyperparameters found were: \", results_nn.get_best_result().config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second test\n",
    "results_nn = run_trials(\n",
    "    mlflow_tracking_uri,\n",
    "    \"nn\",\n",
    "    train_nn,\n",
    "    nn_space,\n",
    "    train_val,\n",
    "    target,\n",
    "    seed_data,\n",
    "    seed_model,\n",
    "    fully_reproducible=False,\n",
    ")\n",
    "print(\"Best hyperparameters found were: \", results_nn.get_best_result().config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_linreg = run_trials(\n",
    "    mlflow_tracking_uri,\n",
    "    \"linreg\",\n",
    "    train_linreg,\n",
    "    linreg_space,\n",
    "    train_val,\n",
    "    target,\n",
    "    seed_data,\n",
    "    seed_model,\n",
    "    fully_reproducible=False,\n",
    ")\n",
    "print(\"Best hyperparameters found were: \", results_linreg.get_best_result().config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raytune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
