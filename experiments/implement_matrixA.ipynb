{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import qiime2 as q2\n",
        "import skbio\n",
        "from classo import classo_problem\n",
        "from qiime2.plugins import phylogeny\n",
        "from skbio import TreeNode\n",
        "from q2_ritme.process_data import load_n_split_data\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_matrix_from_tree(tree):\n",
        "    # Get all leaves and create a mapping from leaf names to indices\n",
        "    leaves = list(tree.tips())\n",
        "    leaf_names = [leaf.name for leaf in leaves]\n",
        "    # map each leaf name to unique index\n",
        "    leaf_index_map = {name: idx for idx, name in enumerate(leaf_names)}\n",
        "\n",
        "    # Get the number of leaves and internal nodes\n",
        "    num_leaves = len(leaf_names)\n",
        "    # root is not included\n",
        "    internal_nodes = list(tree.non_tips())\n",
        "\n",
        "    # Create the identity matrix for the leaves: A1 (num_leaves x num_leaves)\n",
        "    A1 = np.eye(num_leaves)\n",
        "\n",
        "    # Create the matrix for the internal nodes: A2 (num_leaves x\n",
        "    # num_internal_nodes)\n",
        "    # initialise it with zeros\n",
        "    A2 = np.zeros((num_leaves, len(internal_nodes)))\n",
        "\n",
        "    # Populate A2 with 1s for the leaves linked by each internal node\n",
        "    # iterate over all internal nodes to find descendents of this node and mark\n",
        "    # them accordingly\n",
        "    a2_node_names = []\n",
        "    for j, node in enumerate(internal_nodes):\n",
        "        # todo: adjust names to consensus taxonomy from descentents\n",
        "        # for now node names are just increasing integers - since node.name is float\n",
        "        a2_node_names.append(\"n\" + str(j))\n",
        "        descendant_leaves = {leaf.name for leaf in node.tips()}\n",
        "        for leaf_name in leaf_names:\n",
        "            if leaf_name in descendant_leaves:\n",
        "                A2[leaf_index_map[leaf_name], j] = 1\n",
        "\n",
        "    # Concatenate A1 and A2 to create the final matrix A\n",
        "    A = np.hstack((A1, A2))\n",
        "\n",
        "    return A, a2_node_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the tree nodes with lengths\n",
        "n1 = TreeNode(name=\"n1\")\n",
        "f1 = TreeNode(name=\"f1\", length=1.0)\n",
        "f2 = TreeNode(name=\"f2\", length=1.0)\n",
        "n2 = TreeNode(name=\"n2\")\n",
        "f3 = TreeNode(name=\"f3\", length=1.0)\n",
        "\n",
        "# Build the tree structure with lengths\n",
        "n1.extend([f1, f2])\n",
        "n2.extend([n1, f3])\n",
        "n1.length = 1.0\n",
        "n2.length = 1.0\n",
        "\n",
        "# n2 is the root of this tree\n",
        "tree = n2\n",
        "print(tree.ascii_art())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A_example, a2_names_ex = create_matrix_from_tree(tree)\n",
        "A_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a2_names_ex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real data: MA2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read feature table\n",
        "art_feature_table = q2.Artifact.load(\"data/220728_monthly/all_otu_table_filt.qza\")\n",
        "df_ft = art_feature_table.view(pd.DataFrame)\n",
        "df_ft.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_to_taxonomy = \"data/220728_monthly/otu_taxonomy_all.qza\"\n",
        "art_taxonomy = q2.Artifact.load(path_to_taxonomy)\n",
        "df_taxonomy = art_taxonomy.view(pd.DataFrame)\n",
        "print(df_taxonomy.shape)\n",
        "\n",
        "# Filter the taxonomy based on the feature table\n",
        "df_taxonomy_f = df_taxonomy[df_taxonomy.index.isin(df_ft.columns.tolist())]\n",
        "print(df_taxonomy_f.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read silva phylo tree\n",
        "path_to_phylo = \"data/220728_monthly/silva-138-99-rooted-tree.qza\"\n",
        "art_phylo = q2.Artifact.load(path_to_phylo)\n",
        "tree_phylo = art_phylo.view(skbio.TreeNode)\n",
        "# total nodes\n",
        "tree_phylo.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filter tree by feature table: this prunes a phylogenetic tree to match the\n",
        "# input ids\n",
        "(art_phylo_f,) = phylogeny.actions.filter_tree(tree=art_phylo, table=art_feature_table)\n",
        "tree_phylo_f = art_phylo_f.view(skbio.TreeNode)\n",
        "\n",
        "# total nodes\n",
        "tree_phylo_f.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ensure that # leaves in tree == feature table dimension\n",
        "num_leaves = tree_phylo_f.count(tips=True)\n",
        "assert num_leaves == df_ft.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A, a2_names = create_matrix_from_tree(tree_phylo_f)\n",
        "A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a2_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# verification\n",
        "# no all 1 in one column\n",
        "assert not np.any(np.all(A == 1.0, axis=0))\n",
        "\n",
        "# shape should be = feature_count + node_count\n",
        "nb_features = df_ft.shape[1]\n",
        "nb_non_leaf_nodes = len(list(tree_phylo_f.non_tips()))\n",
        "\n",
        "assert nb_features + nb_non_leaf_nodes == A.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run trac with this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load metadata\n",
        "target = \"age_months\"\n",
        "train_val, test = load_n_split_data(\n",
        "    path2md=\"data/220728_monthly/metadata_proc_v20240323_r0_r3_le_2yrs.tsv\",\n",
        "    path2ft=\"data/220728_monthly/all_otu_table_filt.qza\",\n",
        "    host_id=\"host_id\",\n",
        "    target=target,\n",
        "    train_size=0.8,\n",
        "    seed=12,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# preprocess taxonomy aggregation\n",
        "def _preprocess_taxonomy_aggregation(x, A):\n",
        "    pseudo_count = 0.000001\n",
        "    # ? what happens if x is relative abundances\n",
        "    X = np.log(pseudo_count + x)\n",
        "    nleaves = np.sum(A, axis=0)\n",
        "    log_geom = X.dot(A) / nleaves\n",
        "\n",
        "    return log_geom, nleaves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# perform preprocessing on train\n",
        "ft_cols = [x for x in train_val.columns if x.startswith(\"F\")]\n",
        "x_train_val = train_val[ft_cols]\n",
        "y_train_val = train_val[target]\n",
        "# todo: afterwards perform it on test\n",
        "log_geom_trainval, nleaves = _preprocess_taxonomy_aggregation(x_train_val.values, A)\n",
        "\n",
        "n, d = log_geom_trainval.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get labels from taxonomy\n",
        "# change labels to match new feature names\n",
        "df_taxonomy_f.index = df_taxonomy_f.index.map(lambda x: \"F\" + str(x))\n",
        "\n",
        "# todo: add proper A2 labels for A -> for now it's just n + count\n",
        "label = df_taxonomy_f[\"Taxon\"].values\n",
        "label_short = np.array([la.split(\";\")[-1].strip() for la in label])\n",
        "assert len(label) == len(ft_cols)\n",
        "assert len(label) == len(label_short)\n",
        "label = np.append(label, a2_names)\n",
        "label_short = np.append(label_short, a2_names)\n",
        "\n",
        "assert len(label_short) == A.shape[1]\n",
        "label_short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# perform CV classo: trac\n",
        "problem = classo_problem(log_geom_trainval, y_train_val.values, label=label_short)\n",
        "\n",
        "problem.formulation.w = 1 / nleaves\n",
        "problem.formulation.intercept = True\n",
        "problem.formulation.concomitant = False  # not relevant for here\n",
        "\n",
        "# ! one form of model selection needs to be chosen\n",
        "# stability selection: for pre-selected range of lambda find beta paths\n",
        "problem.model_selection.StabSel = False\n",
        "# calculate coefficients for a grid of lambdas\n",
        "problem.model_selection.PATH = False\n",
        "# todo: check if it is fair that trac is trained with CV internally whereas others are not\n",
        "# lambda values checked with CV are `Nlam` points between 1 and `lamin`, with\n",
        "# logarithm scale or not depending on `logscale`.\n",
        "problem.model_selection.CV = True\n",
        "problem.model_selection.CVparameters.seed = (\n",
        "    6  # one could change logscale, Nsubset, oneSE\n",
        ")\n",
        "# 'one-standard-error' = select simplest model (largest lambda value) in CV\n",
        "# whose CV score is within 1 stddev of best score\n",
        "# ! create hyperparameter for this\n",
        "problem.model_selection.CVparameters.oneSE = True\n",
        "# ! create hyperparameter for this\n",
        "problem.model_selection.CVparameters.Nlam = 80\n",
        "# ! create hyperparameter for this\n",
        "problem.model_selection.CVparameters.lamin = 0.001\n",
        "\n",
        "# ! for ritme: no feature_transformation to be used for trac\n",
        "print(problem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "problem.solve()\n",
        "# todo: find out how to extract the insights from the model to disk without changing classo\n",
        "print(problem.solution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# alpha [0] is learned intercept, alpha [1:] are learned coefficients for all features\n",
        "# in logGeom (n_samples, n_features)\n",
        "# ! if oneSE=True -> uses lambda_1SE else lambda_min (see CV in\n",
        "# ! classo>cross_validation.py)\n",
        "# refit -> solves unconstrained least squares problem with selected lambda and\n",
        "# variables\n",
        "alpha = problem.solution.CV.refit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! class solution_CV: defined in @solver.py L930\n",
        "selection = problem.solution.CV.selected_param[1:]  # exclude the intercept\n",
        "selected_ft = label[selection]\n",
        "print(selected_ft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # selected lambda with 1-standard-error method\n",
        "# problem.solution.CV.lambda_1SE\n",
        "\n",
        "# # selected lambda without 1-standard-error method\n",
        "# problem.solution.CV.lambda_min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save model: A, label, alpha (includes selected_ft)\n",
        "path2out = \"test_model\"\n",
        "if not os.path.exists(path2out):\n",
        "    os.makedirs(path2out)\n",
        "\n",
        "# storing A w labels\n",
        "df_A_with_labels = pd.DataFrame(A, columns=label, index=label[:nb_features])\n",
        "df_A_with_labels.to_csv(os.path.join(path2out, \"matrix_a_w_labels.csv\"), index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# storing alpha w labels\n",
        "idx_alpha = [\"intercept\"] + label.tolist()\n",
        "df_alpha_with_labels = pd.DataFrame(alpha, columns=[\"alpha\"], index=idx_alpha)\n",
        "df_alpha_with_labels.to_csv(\n",
        "    os.path.join(path2out, \"model_alpha_w_labels.csv\"), index=True\n",
        ")\n",
        "\n",
        "# we can get selected features from alpha\n",
        "selected_ft_inf = df_alpha_with_labels[\n",
        "    df_alpha_with_labels[\"alpha\"] != 0\n",
        "].index.tolist()\n",
        "assert selected_ft_inf[1:] == selected_ft.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform prediction on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# derive log_geom for test\n",
        "ft_cols = [x for x in test.columns if x.startswith(\"F\")]\n",
        "\n",
        "x_test = test[ft_cols]\n",
        "y_test = test[target]\n",
        "# todo: read A\n",
        "log_geom_test, nleaves = _preprocess_taxonomy_aggregation(x_test.values, A)\n",
        "\n",
        "# apply model to test\n",
        "# todo: read alpha\n",
        "y_test_pred = log_geom_test.dot(alpha[1:]) + alpha[0]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ritme_wclasso",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
