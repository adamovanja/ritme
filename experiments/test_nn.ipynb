{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lightning import LightningModule, Trainer\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import ray\n",
        "from ray import tune, air, init\n",
        "from ray.tune.integration.pytorch_lightning import TuneReportCheckpointCallback\n",
        "from ray.air.integrations.wandb import WandbLoggerCallback\n",
        "import os\n",
        "import dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleNN(LightningModule):\n",
        "    def __init__(self, input_size, hidden_size, learning_rate):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "        self.learning_rate = learning_rate\n",
        "        self.train_loss = 0\n",
        "        self.val_loss = 0\n",
        "        self.train_predictions = []\n",
        "        self.train_targets = []\n",
        "        self.val_predictions = []\n",
        "        self.val_targets = []\n",
        "        self.rmse_train = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = nn.MSELoss()(y_hat, y)\n",
        "        self.train_loss = loss\n",
        "\n",
        "        self.train_predictions.append(y_hat.detach())\n",
        "        self.train_targets.append(y.detach())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = nn.MSELoss()(y_hat, y)\n",
        "        self.val_loss = loss\n",
        "\n",
        "        self.val_predictions.append(y_hat.detach())\n",
        "        self.val_targets.append(y.detach())\n",
        "\n",
        "        self.log(\"val_loss\", loss)\n",
        "\n",
        "        return {\"val_loss\": loss}\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        all_preds_train = torch.cat(self.train_predictions)\n",
        "        all_targets_train = torch.cat(self.train_targets)\n",
        "\n",
        "        self.rmse_train = torch.sqrt(\n",
        "            nn.functional.mse_loss(all_preds_train, all_targets_train)\n",
        "        )\n",
        "        # if self.i == 9:\n",
        "        #     print(f\"rmse_train in train {self.i}: {rmse_train}\", flush=True)\n",
        "        self.train_predictions.clear()\n",
        "        self.train_targets.clear()\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        all_preds_val = torch.cat(self.val_predictions)\n",
        "        all_targets_val = torch.cat(self.val_targets)\n",
        "\n",
        "        rmse_val = torch.sqrt(nn.functional.mse_loss(all_preds_val, all_targets_val))\n",
        "        self.log(\"rmse_val\", rmse_val)\n",
        "\n",
        "        self.log(\"rmse_train\", self.rmse_train)\n",
        "\n",
        "        self.val_predictions.clear()\n",
        "        self.val_targets.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "\n",
        "def train_nn(config, train_data, val_data):\n",
        "    model = SimpleNN(\n",
        "        input_size=10, hidden_size=config[\"hidden_size\"], learning_rate=config[\"lr\"]\n",
        "    )\n",
        "\n",
        "    # Generate dummy data\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=800)\n",
        "    val_loader = DataLoader(val_data, batch_size=800)\n",
        "\n",
        "    checkpoint_dir = ray.train.get_context().get_trial_dir()\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=10,\n",
        "        num_sanity_val_steps=0,\n",
        "        check_val_every_n_epoch=1,\n",
        "        val_check_interval=1,\n",
        "        callbacks=[  # ModelCheckpoint(\n",
        "            #     monitor=\"rmse_val\",\n",
        "            #     mode=\"min\",\n",
        "            #     save_top_k=1,\n",
        "            #     save_weights_only=False,\n",
        "            #     dirpath=checkpoint_dir,\n",
        "            #     filename=\"{epoch}-{val_rmse:.2f}\",\n",
        "            # ),\n",
        "            TuneReportCheckpointCallback(\n",
        "                metrics={\n",
        "                    \"loss\": \"val_loss\",\n",
        "                    \"rmse_train\": \"rmse_train\",\n",
        "                    \"rmse_val\": \"rmse_val\",\n",
        "                },\n",
        "                filename=\"checkpoint\",\n",
        "                # train_end:\n",
        "                # rmse_train & val_loss missing\n",
        "                # val_rmse correct, train_rmse wrong\n",
        "                # validation_end:\n",
        "                # rmse_train & val_loss missing\n",
        "                # val_rmse correct, train_rmse wrong\n",
        "                #\n",
        "                # validation_epoch_end:\n",
        "                # val_loss, rmse_train & rmse_val missing\n",
        "                # val_rmse & train_rmse wrong\n",
        "                on=\"validation_end\",\n",
        "                # ! added\n",
        "                save_checkpoints=True,\n",
        "            )\n",
        "        ],\n",
        "        # deterministic=True,\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"tuneStatus\">\n",
              "  <div style=\"display: flex;flex-direction: row\">\n",
              "    <div style=\"display: flex;flex-direction: column;\">\n",
              "      <h3>Tune Status</h3>\n",
              "      <table>\n",
              "<tbody>\n",
              "<tr><td>Current time:</td><td>2024-08-23 21:56:39</td></tr>\n",
              "<tr><td>Running for: </td><td>00:00:13.90        </td></tr>\n",
              "<tr><td>Memory:      </td><td>10.4/16.0 GiB      </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "    <div class=\"systemInfo\">\n",
              "      <h3>System Info</h3>\n",
              "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/8 CPUs, 0/0 GPUs\n",
              "    </div>\n",
              "    \n",
              "  </div>\n",
              "  <div class=\"hDivider\"></div>\n",
              "  <div class=\"trialStatus\">\n",
              "    <h3>Trial Status</h3>\n",
              "    <table>\n",
              "<thead>\n",
              "<tr><th>Trial name          </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  rmse_train</th><th style=\"text-align: right;\">  rmse_val</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_nn_c7318_00000</td><td>TERMINATED</td><td>127.0.0.1:14560</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.00115049</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1.42068</td><td style=\"text-align: right;\">8.79886 </td><td style=\"text-align: right;\">     3.01525</td><td style=\"text-align: right;\">  2.96629 </td></tr>\n",
              "<tr><td>train_nn_c7318_00001</td><td>TERMINATED</td><td>127.0.0.1:14561</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">0.02213   </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1.39823</td><td style=\"text-align: right;\">0.384783</td><td style=\"text-align: right;\">     1.02456</td><td style=\"text-align: right;\">  0.620309</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "  </div>\n",
              "</div>\n",
              "<style>\n",
              ".tuneStatus {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".tuneStatus .systemInfo {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              ".tuneStatus .trialStatus {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".tuneStatus .hDivider {\n",
              "  border-bottom-width: var(--jp-border-width);\n",
              "  border-bottom-color: var(--jp-border-color0);\n",
              "  border-bottom-style: solid;\n",
              "}\n",
              ".tuneStatus .vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_nn pid=14561)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m Missing logger folder: /private/tmp/ray/session_2024-08-23_21-56-19_562368_14499/artifacts/2024-08-23_21-56-25/train_nn_2024-08-23_21-56-25/working_dirs/train_nn_c7318_00001_1_hidden_size=32,lr=0.0221_2024-08-23_21-56-26/lightning_logs\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m \n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m   | Name  | Type       | Params | Mode \n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m ---------------------------------------------\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m 0 | model | Sequential | 385    | train\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m ---------------------------------------------\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m 385       Trainable params\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m 385       Total params\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m 0.002     Total estimated model params size (MB)\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m /Users/adamova/miniforge3/envs/ritme/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m /Users/adamova/miniforge3/envs/ritme/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m /Users/adamova/miniforge3/envs/ritme/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m \n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m 0 | model | Sequential | 1.5 K  | train\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m 1.5 K     Trainable params\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m 1.5 K     Total params\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] \n",
            "Epoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.93it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 56.56it/s]\u001b[A\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_nn pid=14561)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/adamova/ray_results/train_nn_2024-08-23_21-56-25/train_nn_c7318_00001_1_hidden_size=32,lr=0.0221_2024-08-23_21-56-26/checkpoint_000000)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_nn pid=14561)\u001b[0m \n",
            "Epoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 75.60it/s, v_num=0]       \u001b[A\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m \n",
            "Epoch 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 110.83it/s, v_num=0]       \u001b[A\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m \n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m \n",
            "Epoch 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 14.04it/s, v_num=0]        \u001b[A\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m \n",
            "Epoch 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 14.07it/s, v_num=0]        \u001b[A\n",
            "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0]        \n",
            "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0]        \n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m \n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m \n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m \n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m \n",
            "Epoch 6: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 11.53it/s, v_num=0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_nn pid=14561)\u001b[0m `Trainer.fit` stopped: `max_epochs=10` reached.\n",
            "2024-08-23 21:56:39,933\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/adamova/ray_results/train_nn_2024-08-23_21-56-25' in 0.0137s.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_nn pid=14561)\u001b[0m \n",
            "Epoch 9: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.86it/s, v_num=0]        \u001b[A\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m \n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_nn pid=14561)\u001b[0m /Users/adamova/miniforge3/envs/ritme/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
            "\u001b[36m(train_nn pid=14561)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: Currently logged in as: adamovanja (ritme). Use `wandb login --relogin` to force relogin\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m Missing logger folder: /private/tmp/ray/session_2024-08-23_21-56-19_562368_14499/artifacts/2024-08-23_21-56-25/train_nn_2024-08-23_21-56-25/working_dirs/train_nn_c7318_00000_0_hidden_size=128,lr=0.0012_2024-08-23_21-56-26/lightning_logs\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: wandb version 0.17.7 is available!  To upgrade, please run:\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: Tracking run with wandb version 0.17.2\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-08-23_21-56-19_562368_14499/artifacts/2024-08-23_21-56-25/train_nn_2024-08-23_21-56-25/driver_artifacts/train_nn_c7318_00001_1_hidden_size=32,lr=0.0221_2024-08-23_21-56-26/wandb/run-20240823_215640-c7318_00001\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: Syncing run train_nn_c7318_00001\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: \u2b50\ufe0f View project at https://wandb.ai/ritme/ray_debug\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: \ud83d\ude80 View run at https://wandb.ai/ritme/ray_debug/runs/c7318_00001\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m   | Name  | Type       | Params | Mode \n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m ---------------------------------------------\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m 0.006     Total estimated model params size (MB)\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m /Users/adamova/miniforge3/envs/ritme/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m /Users/adamova/miniforge3/envs/ritme/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m /Users/adamova/miniforge3/envs/ritme/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/adamova/ray_results/train_nn_2024-08-23_21-56-25/train_nn_c7318_00000_0_hidden_size=128,lr=0.0012_2024-08-23_21-56-26/checkpoint_000009)\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m `Trainer.fit` stopped: `max_epochs=10` reached.\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m /Users/adamova/miniforge3/envs/ritme/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
            "\u001b[36m(train_nn pid=14560)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
            "\u001b[36m(_WandbLoggingActor pid=14567)\u001b[0m wandb: Currently logged in as: adamovanja (ritme). Use `wandb login --relogin` to force relogin\n",
            "wandb:                                                                                \n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: \n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: Run history:\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: iterations_since_restore \u2581\u2582\u2583\u2583\u2584\u2585\u2586\u2586\u2587\u2588\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:                     loss \u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2582\u2581\u2581\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:               rmse_train \u2581\u2588\u2588\u2587\u2587\u2586\u2586\u2585\u2584\u2583\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:                 rmse_val \u2588\u2587\u2587\u2586\u2585\u2584\u2583\u2582\u2581\u2581\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:       time_since_restore \u2581\u2582\u2583\u2583\u2584\u2585\u2585\u2586\u2587\u2588\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:         time_this_iter_s \u2588\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:             time_total_s \u2581\u2582\u2583\u2583\u2584\u2585\u2585\u2586\u2587\u2588\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:                timestamp \u2581\u2585\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:       training_iteration \u2581\u2582\u2583\u2583\u2584\u2585\u2586\u2586\u2587\u2588\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: \n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: Run summary:\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: iterations_since_restore 10\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:                     loss 0.38478\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:               rmse_train 1.02456\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:                 rmse_val 0.62031\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:       time_since_restore 1.39823\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:         time_this_iter_s 0.04948\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:             time_total_s 1.39823\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:                timestamp 1724442999\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb:       training_iteration 10\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: \n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: \ud83d\ude80 View run train_nn_c7318_00001 at: https://wandb.ai/ritme/ray_debug/runs/c7318_00001\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: \u2b50\ufe0f View project at: https://wandb.ai/ritme/ray_debug\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: Find logs at: ./wandb/run-20240823_215640-c7318_00001/logs\n",
            "\u001b[36m(_WandbLoggingActor pid=14564)\u001b[0m wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
            "\u001b[36m(_WandbLoggingActor pid=14567)\u001b[0m wandb:                     loss \u2588\u2587\u2586\u2586\u2585\u2584\u2583\u2583\u2582\u2581\n",
            "\u001b[36m(_WandbLoggingActor pid=14567)\u001b[0m wandb:               rmse_train \u2581\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
            "\u001b[36m(_WandbLoggingActor pid=14567)\u001b[0m wandb:                 rmse_val \u2588\u2587\u2586\u2586\u2585\u2584\u2583\u2583\u2582\u2581\n",
            "\u001b[36m(_WandbLoggingActor pid=14567)\u001b[0m wandb:       time_since_restore \u2581\u2582\u2582\u2583\u2584\u2585\u2585\u2586\u2587\u2588\n",
            "\u001b[36m(_WandbLoggingActor pid=14567)\u001b[0m wandb:             time_total_s \u2581\u2582\u2582\u2583\u2584\u2585\u2585\u2586\u2587\u2588\n",
            "\u001b[36m(_WandbLoggingActor pid=14567)\u001b[0m wandb:                timestamp \u2581\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
            "2024-08-23 21:56:49,000\tINFO tune.py:1041 -- Total run time: 23.81 seconds (13.88 seconds for the tuning loop).\n"
          ]
        }
      ],
      "source": [
        "config = {\"lr\": tune.loguniform(1e-4, 1e-1), \"hidden_size\": tune.choice([32, 64, 128])}\n",
        "\n",
        "context = init(\n",
        "    address=\"local\",\n",
        "    include_dashboard=False,\n",
        "    ignore_reinit_error=True,\n",
        ")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "X = torch.randn(1000, 10)\n",
        "y = torch.sum(X, dim=1, keepdim=True)\n",
        "\n",
        "train_data = TensorDataset(X[:800], y[:800])\n",
        "val_data = TensorDataset(X[800:], y[800:])\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "entity = os.getenv(\"WANDB_ENTITY\")\n",
        "callback = [WandbLoggerCallback(api_key=api_key, entity=entity, project=\"ray_debug\")]\n",
        "tuner = tune.Tuner(\n",
        "    tune.with_parameters(train_nn, train_data=train_data, val_data=val_data),\n",
        "    tune_config=tune.TuneConfig(metric=\"rmse_val\", mode=\"min\", num_samples=2),\n",
        "    param_space=config,\n",
        "    run_config=air.RunConfig(\n",
        "        callbacks=callback,\n",
        "    ),\n",
        ")\n",
        "\n",
        "results = tuner.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial config: {'lr': 0.022130015984348436, 'hidden_size': 32}\n",
            "Best trial final train rmse: 1.0245614051818848\n",
            "Best trial final validation rmse: 0.6203088164329529\n"
          ]
        }
      ],
      "source": [
        "best_result = results.get_best_result(\"rmse_val\", \"min\", scope=\"all\")\n",
        "print(f\"Best trial config: {best_result.config}\")\n",
        "best_rmse_train = best_result.metrics[\"rmse_train\"]\n",
        "best_rmse_val = best_result.metrics[\"rmse_val\"]\n",
        "print(f\"Best trial final train rmse: {best_rmse_train}\")\n",
        "print(f\"Best trial final validation rmse: {best_rmse_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_size': 10, 'hidden_size': 32, 'learning_rate': 0.022130015984348436}\n"
          ]
        }
      ],
      "source": [
        "# checkpoint_dir = best_result.checkpoint.to_directory()\n",
        "# best_result.checkpoint\n",
        "checkpoint_dir = best_result.checkpoint.path\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
        "print(checkpoint[\"hyper_parameters\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6273286938667297\n",
            "0.6203088164329529\n"
          ]
        }
      ],
      "source": [
        "model = SimpleNN.load_from_checkpoint(checkpoint_path)\n",
        "\n",
        "# train\n",
        "# model.eval()\n",
        "# train_data = TensorDataset(X[:800], y[:800])\n",
        "rmse_train_recalc = torch.sqrt(nn.functional.mse_loss(model(X[:800]), y[:800])).item()\n",
        "# rmse_train_recalc = root_mean_squared_error(model(X[:800]).detach().numpy(), y[:800].detach().numpy())\n",
        "print(rmse_train_recalc)\n",
        "\n",
        "# val\n",
        "# model.eval()\n",
        "rmse_val_calc = torch.sqrt(nn.functional.mse_loss(model(X[800:]), y[800:])).item()\n",
        "# rmse_val_calc = root_mean_squared_error(model(X[800:]).detach().numpy(), y[800:].detach().numpy())\n",
        "print(rmse_val_calc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_loader = DataLoader(train_data, batch_size=32)\n",
        "# val_loader = DataLoader(val_data, batch_size=32)\n",
        "\n",
        "# model.eval()\n",
        "\n",
        "# train_preds = []\n",
        "# train_targets = []\n",
        "# for batch in train_loader:\n",
        "#     x, y = batch\n",
        "#     preds = model(x)\n",
        "#     train_preds.append(preds)\n",
        "#     train_targets.append(y)\n",
        "\n",
        "# train_preds = torch.cat(train_preds)\n",
        "# train_targets = torch.cat(train_targets)\n",
        "# rmse_train_recalc = torch.sqrt(nn.functional.mse_loss(train_preds, train_targets)).item()\n",
        "# print(rmse_train_recalc)\n",
        "# val_preds = []\n",
        "# val_targets = []\n",
        "# for batch in val_loader:\n",
        "#     x, y = batch\n",
        "#     preds = model(x)\n",
        "#     val_preds.append(preds)\n",
        "#     val_targets.append(y)\n",
        "\n",
        "# val_preds = torch.cat(val_preds)\n",
        "# val_targets = torch.cat(val_targets)\n",
        "# rmse_val_calc = torch.sqrt(nn.functional.mse_loss(val_preds, val_targets)).item()\n",
        "# print(rmse_val_calc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ritme",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
