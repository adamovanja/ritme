{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore fully parallel launching of all trainables\n",
        "this is option 1 in below conversation with Claude Opus from Phind:\n",
        "\n",
        "**Me: what is the recommended way of launching multiple trainables? option 1 or option 2? and why?**\n",
        "\n",
        "\n",
        "**Claude:** Both options have their merits, but I would recommend using Option 2 for launching multiple trainables. Here's why:\n",
        "\n",
        "Separation of search spaces: Option 2 defines separate search spaces for each model type (rf_search_space and linreg_search_space). This approach allows for cleaner and more explicit definition of the hyperparameters specific to each model type. It makes the code more readable and maintainable, especially if you have multiple model types with different sets of hyperparameters.\n",
        "Flexibility in tuning: With Option 2, you have the flexibility to run separate tuning sessions for each model type. This allows you to control the tuning process independently for each model type. You can easily modify the tuning configuration (e.g., num_samples, metric, mode) for each model type if needed. It also enables you to run the tuning sessions in parallel or sequentially, depending on your requirements.\n",
        "Clarity in results: Option 2 provides separate reporting of the best hyperparameters and best metric value for each model type. This makes it easier to analyze and compare the results of different model types. You can clearly see the best configuration and performance for each model type, which can help in making informed decisions about which model to choose for your specific task.\n",
        "Scalability: Option 2 is more scalable when you have multiple model types or want to add new model types in the future. You can easily add new search spaces for additional model types without modifying the existing code. This modularity and extensibility make it easier to maintain and expand your codebase.\n",
        "In contrast, Option 1 combines the search spaces for all model types into a single search space. While this approach can work, it may become less manageable as the number of model types and hyperparameters grows. It also doesn't provide the same level of flexibility and clarity in tuning and reporting results for each model type separately.\n",
        "\n",
        "However, it's worth noting that Option 1 can be suitable if you have a small number of model types and want to perform a single tuning session across all model types. It may be more concise and straightforward in such cases.\n",
        "\n",
        "Ultimately, the choice between Option 1 and Option 2 depends on your specific requirements, the complexity of your model types, and the desired level of control and clarity in the tuning process. In general, Option 2 offers more flexibility, modularity, and clarity, making it the recommended approach for launching multiple trainables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import ElasticNet\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.air import session\n",
        "\n",
        "\n",
        "def train_model(config):\n",
        "    data = fetch_california_housing()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data.data, data.target, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    if config[\"model_type\"] == \"rf\":\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=config[\"n_estimators\"],\n",
        "            max_depth=config[\"max_depth\"],\n",
        "            min_samples_split=config[\"min_samples_split\"],\n",
        "            min_samples_leaf=config[\"min_samples_leaf\"],\n",
        "        )\n",
        "    else:  # config[\"model_type\"] == \"linreg\"\n",
        "        model = ElasticNet(\n",
        "            alpha=config[\"alpha\"],\n",
        "            l1_ratio=config[\"l1_ratio\"],\n",
        "            fit_intercept=config[\"fit_intercept\"],\n",
        "        )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    session.report({\"mse\": mse})\n",
        "\n",
        "\n",
        "ray.init(ignore_reinit_error=True)\n",
        "search_space = {\n",
        "    \"model_type\": tune.grid_search([\"rf\", \"linreg\"]),\n",
        "    # rf\n",
        "    \"n_estimators\": tune.choice([1, 2]),\n",
        "    \"max_depth\": tune.choice([None, 1, 2]),\n",
        "    \"min_samples_split\": tune.choice([2, 5, 10]),\n",
        "    \"min_samples_leaf\": tune.choice([1, 2, 4]),\n",
        "    # linreg\n",
        "    \"alpha\": tune.uniform(0.0, 1.0),\n",
        "    \"l1_ratio\": tune.uniform(0.0, 1.0),\n",
        "    \"fit_intercept\": tune.choice([True, False]),\n",
        "}\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    train_model,\n",
        "    param_space=search_space,\n",
        "    tune_config=tune.TuneConfig(\n",
        "        num_samples=2,\n",
        "        metric=\"mse\",\n",
        "        mode=\"min\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "results = tuner.fit()\n",
        "best_result = results.get_best_result(metric=\"mse\", mode=\"min\")\n",
        "print(f\"Best hyperparameters: {best_result.config}\")\n",
        "print(f\"Best MSE: {best_result.metrics['mse']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ritme",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
