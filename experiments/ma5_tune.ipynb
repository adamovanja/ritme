{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try out framework with 5-cohort MA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from q2_time.process_data import load_n_split_data\n",
    "from q2_time.tune_models import run_all_trials\n",
    "from q2_time.evaluate_models import get_best_model, get_best_data_processing\n",
    "from q2_time.engineer_features import transform_features\n",
    "from q2_time.config import HOST_ID, TARGET\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# 30.437 is avg. number of days per month\n",
    "DAYS_PER_MONTH = 30.437\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/220728_monthly\"\n",
    "path2md = os.path.join(data_dir, \"metadata_proc_v20230824.tsv\")\n",
    "path2ft = os.path.join(data_dir, \"all_otu_table.qza\")\n",
    "train_val, test = load_n_split_data(path2md, path2ft, [HOST_ID, TARGET])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dic = run_all_trials(train_val, model_types=[\"xgb\", \"nn\", \"linreg\", \"rf\"])\n",
    "result_dic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunedModel:\n",
    "    def __init__(self, model, data_config):\n",
    "        self.model = model\n",
    "        self.data_config = data_config\n",
    "\n",
    "    def transform(self, data):\n",
    "        transformed = transform_features(\n",
    "            data,\n",
    "            self.data_config[\"data_transform\"],\n",
    "            self.data_config[\"data_alr_denom_idx\"],\n",
    "        )\n",
    "        if isinstance(self.model, xgb.core.Booster):\n",
    "            return xgb.DMatrix(transformed)\n",
    "        else:\n",
    "            return transformed.values\n",
    "\n",
    "    def predict(self, data):\n",
    "        transformed = self.transform(data)\n",
    "        predicted = self.model.predict(transformed)\n",
    "        return predicted.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dic = {}\n",
    "for model_type, result_grid in result_dic.items():\n",
    "    best_model = get_best_model(model_type, result_grid)\n",
    "    best_data_proc = get_best_data_processing(result_grid)\n",
    "\n",
    "    best_model_dic[model_type] = TunedModel(best_model, best_data_proc)\n",
    "\n",
    "best_model_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Perform predictions with best model\n",
    "def save_predictions(data, tmodel, target, features, split=None):\n",
    "    # id, true\n",
    "    saved_pred = data[[target]].copy()\n",
    "    saved_pred.rename(columns={target: \"true\"}, inplace=True)\n",
    "    # pred, split\n",
    "    saved_pred[\"pred\"] = tmodel.predict(data[features])\n",
    "    saved_pred[\"split\"] = split\n",
    "    return saved_pred\n",
    "\n",
    "\n",
    "non_features = [TARGET, HOST_ID]\n",
    "features = [x for x in train_val if x not in non_features]\n",
    "\n",
    "# dic with model_type: df with all predictions within\n",
    "preds_dic = {}\n",
    "for model_type, tmodel in best_model_dic.items():\n",
    "    train_df = save_predictions(train_val, tmodel, TARGET, features, \"train\")\n",
    "    test_pred = save_predictions(test, tmodel, TARGET, features, \"test\")\n",
    "\n",
    "    preds_dic[model_type] = pd.concat([train_df, test_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rmse over all experiments # !(OVERALL case)\n",
    "def _calculate_rmse(pred_df):\n",
    "    rmse_scores = {}\n",
    "    for split in pred_df[\"split\"].unique():\n",
    "        pred_split = pred_df[pred_df[\"split\"] == split].copy()\n",
    "        rmse = mean_squared_error(\n",
    "            pred_split[\"true\"].values, pred_split[\"pred\"].values, squared=False\n",
    "        )\n",
    "        rmse_scores[split] = rmse\n",
    "    return rmse_scores\n",
    "\n",
    "\n",
    "rmse_dic = {}\n",
    "for model_type, pred_df in preds_dic.items():\n",
    "    # todo: add a bin_by variable (for \"over time\" and \"over study\")\n",
    "    rmse_dic[model_type] = _calculate_rmse(pred_df)\n",
    "\n",
    "rmse_df = pd.DataFrame(rmse_dic).T\n",
    "rmse_df.plot(kind=\"bar\", title=\"Overall\", ylabel=\"RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rmse over true time bin\n",
    "# ! (TIME case)\n",
    "model_type = \"rf\"\n",
    "pred_df = preds_dic[model_type]\n",
    "split = None\n",
    "\n",
    "# bin true columns by months\n",
    "pred_df[\"group\"] = np.round(pred_df[\"true\"] / DAYS_PER_MONTH, 0).astype(int)\n",
    "\n",
    "grouped_ser = pred_df.groupby([\"group\"]).apply(_calculate_rmse)\n",
    "grouped_df = grouped_ser.apply(pd.Series)\n",
    "if split is not None:\n",
    "    grouped_df = grouped_df[[split]].copy()\n",
    "\n",
    "grouped_df.plot(\n",
    "    kind=\"bar\", title=f\"Model: {model_type}\", ylabel=\"RMSE\", figsize=(10, 5)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(exp_name, trial_result, metric_ls=[\"rmse_train\", \"rmse_val\"]):\n",
    "    # Get the result with the metric and mode defined in tune_config before\n",
    "    best_result = trial_result.get_best_result()\n",
    "    config = best_result.config\n",
    "\n",
    "    # # get config of best model\n",
    "    # best_result.config\n",
    "\n",
    "    metrics_ser = best_result.metrics_dataframe[metric_ls].iloc[-1]\n",
    "    metrics_df = pd.DataFrame({exp_name: metrics_ser})\n",
    "    return metrics_df, config\n",
    "\n",
    "\n",
    "def calc_best_metrics(dic_trials):\n",
    "    df_metrics = pd.DataFrame(index=[\"rmse_train\", \"rmse_val\"])\n",
    "    dic_config = {}\n",
    "    for key, value in dic_trials.items():\n",
    "        df_best, config = get_best_model(key, value)\n",
    "        df_metrics = df_metrics.join(df_best)\n",
    "        dic_config[key] = config\n",
    "\n",
    "    return df_metrics, pd.DataFrame(dic_config)\n",
    "\n",
    "\n",
    "def plot_best_metrics(df_metrics):\n",
    "    df2plot = df_metrics.T.sort_values(\"rmse_val\", ascending=True)\n",
    "    df2plot.columns = [\"train\", \"validation\"]\n",
    "    # plot settings\n",
    "    # todo: set default plot settings across package\n",
    "    plt.style.use(\"seaborn-v0_8-colorblind\")  # (\"tableau-colorblind10\")\n",
    "    titlesize = 14\n",
    "    labelsize = 13\n",
    "    ticklabel = 12\n",
    "    plt.rcParams.update({\"font.size\": labelsize})\n",
    "\n",
    "    df2plot.plot(kind=\"bar\", figsize=(12, 6))\n",
    "\n",
    "    plt.xticks(fontsize=ticklabel)\n",
    "    plt.yticks(fontsize=ticklabel)\n",
    "    plt.ylabel(\"RMSE\", fontsize=labelsize)\n",
    "    plt.xlabel(\"Model type (order: increasing val score)\", fontsize=labelsize)\n",
    "    plt.title(\"Metrics comparison\", fontsize=titlesize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all, best_configs = calc_best_metrics(\n",
    "    {\n",
    "        \"xgb\": result_dic[\"xgb\"],\n",
    "        \"linreg\": result_dic[\"linreg\"],\n",
    "        \"nn\": result_dic[\"nn\"],\n",
    "        \"rf\": result_dic[\"rf\"],\n",
    "    }\n",
    ")\n",
    "plot_best_metrics(metrics_all)\n",
    "display(best_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _highlight_differing_cols(x):\n",
    "    \"\"\"\n",
    "    Function returning color map of differing columns in x\n",
    "    Original code used as base:\n",
    "    https://stackoverflow.com/questions/41654949/pandas-style-function-to-highlight-specific-columns\n",
    "    \"\"\"\n",
    "    # copy df to new - original data is not changed\n",
    "    df = x.copy()\n",
    "\n",
    "    # extract list of columns that differ between all models\n",
    "    ls_col = df.columns[df.nunique(dropna=False) > 1].tolist()\n",
    "\n",
    "    # select default neutral background\n",
    "    df.loc[:, :] = \"background-color: None\"\n",
    "\n",
    "    # mark columns that differ\n",
    "    df[ls_col] = \"color: red\"\n",
    "\n",
    "    # return colored df\n",
    "    return df\n",
    "\n",
    "\n",
    "best_configs.style.apply(_highlight_differing_cols, axis=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate over training time (example for xgb model here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the result with the maximum test set `mean_accuracy`\n",
    "best_xgb = result_dic[\"xgb\"].get_best_result()\n",
    "best_xgb.metrics_dataframe.plot(\"training_iteration\", [\"rmse_train\", \"rmse_val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = None\n",
    "for result in result_dic[\"xgb\"]:\n",
    "    label = f\"data_transform={result.config['data_transform']}, \\\n",
    "        max_depth={result.config['max_depth']}\"\n",
    "    if ax is None:\n",
    "        ax = result.metrics_dataframe.plot(\n",
    "            \"training_iteration\", \"rmse_val\", label=label\n",
    "        )\n",
    "    else:\n",
    "        result.metrics_dataframe.plot(\n",
    "            \"training_iteration\", \"rmse_val\", ax=ax, label=label\n",
    "        )\n",
    "ax.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "\n",
    "ax.set_title(\"rsme_val vs. training iteration for all trials\")\n",
    "ax.set_ylabel(\"RMSE_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raytune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
