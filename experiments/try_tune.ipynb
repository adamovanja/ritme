{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Try out framework with simulated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from ray.tune.analysis import ExperimentAnalysis\n",
        "\n",
        "from q2_ritme.config import (\n",
        "    HOST_ID,\n",
        "    MLFLOW_TRACKING_URI,\n",
        "    SEED_DATA,\n",
        "    SEED_MODEL,\n",
        "    TARGET,\n",
        "    TRAIN_SIZE,\n",
        ")\n",
        "from q2_ritme.evaluate_models import (\n",
        "    aggregate_best_models_metrics_and_configs,\n",
        "    get_predictions,\n",
        "    plot_best_models_comparison,\n",
        "    plot_model_training_over_iterations,\n",
        "    plot_rmse_over_experiments,\n",
        "    plot_rmse_over_time,\n",
        "    retrieve_best_models,\n",
        ")\n",
        "from q2_ritme.process_data import load_n_split_data\n",
        "from q2_ritme.tune_models import run_all_trials\n",
        "\n",
        "# 30.437 is avg. number of days per month\n",
        "DAYS_PER_MONTH = 30.437\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ls_model_types = [\"nn\", \"xgb\", \"linreg\", \"rf\"]\n",
        "experiment_tag = \"test_synthetic\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_val, test = load_n_split_data(None, None, HOST_ID, TARGET, TRAIN_SIZE, SEED_DATA)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run all experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_dic = run_all_trials(\n",
        "    train_val,\n",
        "    TARGET,\n",
        "    HOST_ID,\n",
        "    SEED_DATA,\n",
        "    SEED_MODEL,\n",
        "    MLFLOW_TRACKING_URI,\n",
        "    experiment_tag,\n",
        "    model_types=ls_model_types,\n",
        "    fully_reproducible=False,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate best models: train_val vs. test - performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison_output = os.path.join(\"best_models\", experiment_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_dic = retrieve_best_models(result_dic)\n",
        "\n",
        "# Assuming best_model_dic and the datasets are available\n",
        "non_features = [TARGET, HOST_ID]\n",
        "features = [x for x in train_val if x not in non_features]\n",
        "\n",
        "preds_dic = {}\n",
        "for model_type, tmodel in best_model_dic.items():\n",
        "    train_pred = get_predictions(train_val, tmodel, TARGET, features, \"train\")\n",
        "    test_pred = get_predictions(test, tmodel, TARGET, features, \"test\")\n",
        "    all_pred = pd.concat([train_pred, test_pred])\n",
        "\n",
        "    # save all predictions to model file\n",
        "    path2save = os.path.join(tmodel.path, \"predictions.csv\")\n",
        "    all_pred.to_csv(path2save, index=True)\n",
        "    preds_dic[model_type] = all_pred\n",
        "\n",
        "plot_rmse_over_experiments(preds_dic, comparison_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_rmse_over_time(preds_dic, ls_model_types, DAYS_PER_MONTH, comparison_output)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate best models: train vs. val - performance and config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_all, best_configs = aggregate_best_models_metrics_and_configs(result_dic)\n",
        "plot_best_models_comparison(metrics_all, comparison_output)\n",
        "display(best_configs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate one model over iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_model_training_over_iterations(\"xgb\", result_dic, labels=[\"data_transform\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Postrun evaluation over all experiments performed\n",
        "experiment > trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _min_comparison(current, best):\n",
        "    return current < best\n",
        "\n",
        "\n",
        "def _max_comparison(current, best):\n",
        "    return current > best\n",
        "\n",
        "\n",
        "def best_trial_name(analyses_ls, metric_to_evaluate, mode=\"min\"):\n",
        "    best_trial_overall = None\n",
        "\n",
        "    if mode == \"min\":\n",
        "        best_metric = float(\"inf\")\n",
        "        comparison_operator = _min_comparison\n",
        "    else:\n",
        "        best_metric = -float(\"inf\")\n",
        "        comparison_operator = _max_comparison\n",
        "\n",
        "    for analysis in analyses_ls:\n",
        "        # Get the best trial for the current analysis based on the metric\n",
        "        best_trial = analysis.get_best_trial(metric_to_evaluate, mode, \"all\")\n",
        "\n",
        "        # Retrieve the best metric for this trial\n",
        "        best_trial_metric = best_trial.metric_analysis[metric_to_evaluate][mode]\n",
        "\n",
        "        # Update the overall best trial if this trial has a better \"trial_metric\"\n",
        "        if comparison_operator(best_trial_metric, best_metric):\n",
        "            best_trial_overall = best_trial\n",
        "            best_metric = best_trial_metric\n",
        "\n",
        "    return best_trial_overall\n",
        "\n",
        "\n",
        "def get_all_exp_analyses(experiment_dir):\n",
        "    state_files = glob.glob(os.path.join(experiment_dir, \"experiment_state-*.json\"))\n",
        "    analyses_ls = []\n",
        "    for f in state_files:\n",
        "        analyses_ls.append(ExperimentAnalysis(experiment_checkpoint_path=f))\n",
        "    return analyses_ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read all ExperimentAnalysis objects from this directory\n",
        "best_trials_overall = {}\n",
        "for model in ls_model_types:\n",
        "    experiment_dir = f\"best_models/{experiment_tag}/{model}\"\n",
        "    analyses_ls = get_all_exp_analyses(experiment_dir)\n",
        "\n",
        "    # identify best trial from all analyses of this model type\n",
        "    best_trials_overall[model] = best_trial_name(analyses_ls, \"rmse_val\", mode=\"min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_trials_overall\n",
        "\n",
        "## note you can retrieve config of each trial from the values, e.g.\n",
        "# best_trials_overall[\"linreg\"].config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# next function would be: compare_trials(trial_id1, trial_id2)\n",
        "# -> this function would load the saved train, test val predictions + the model\n",
        "# config (config.json)\n",
        "# TODO: create this"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "raytune",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
