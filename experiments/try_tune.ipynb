{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Try out framework with simulated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: delete this notebook (deprecated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from q2_ritme.config import (\n",
        "    experiment_tag,\n",
        "    host_id,\n",
        "    ls_model_types,\n",
        "    mlflow_tracking_uri,\n",
        "    path_to_ft,\n",
        "    path_to_md,\n",
        "    seed_data,\n",
        "    seed_model,\n",
        "    target,\n",
        "    train_size,\n",
        ")\n",
        "from q2_ritme.evaluate_all_experiments import (\n",
        "    best_trial_name,\n",
        "    compare_trials,\n",
        "    get_all_exp_analyses,\n",
        ")\n",
        "from q2_ritme.evaluate_models import (\n",
        "    aggregate_best_models_metrics_and_configs,\n",
        "    get_predictions,\n",
        "    plot_best_models_comparison,\n",
        "    plot_model_training_over_iterations,\n",
        "    plot_rmse_over_experiments,\n",
        "    plot_rmse_over_time,\n",
        "    retrieve_best_models,\n",
        ")\n",
        "from q2_ritme.process_data import load_n_split_data\n",
        "from q2_ritme.tune_models import run_all_trials\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_val, test = load_n_split_data(\n",
        "    path_to_md, path_to_ft, host_id, target, train_size, seed_data\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run all experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exp_comparison_output = os.path.join(\"best_models\", experiment_tag)\n",
        "\n",
        "if os.path.exists(exp_comparison_output):\n",
        "    raise ValueError(\n",
        "        f\"This experiment tag already exists: {experiment_tag}. Please use another one.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_dic = run_all_trials(\n",
        "    train_val,\n",
        "    target,\n",
        "    host_id,\n",
        "    seed_data,\n",
        "    seed_model,\n",
        "    mlflow_tracking_uri,\n",
        "    experiment_tag,\n",
        "    model_types=ls_model_types,\n",
        "    fully_reproducible=False,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate best models: train_val vs. test - performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_dic = retrieve_best_models(result_dic)\n",
        "\n",
        "# Assuming best_model_dic and the datasets are available\n",
        "non_features = [target, host_id]\n",
        "features = [x for x in train_val if x not in non_features]\n",
        "\n",
        "preds_dic = {}\n",
        "for model_type, tmodel in best_model_dic.items():\n",
        "    train_pred = get_predictions(train_val, tmodel, target, features, \"train\")\n",
        "    test_pred = get_predictions(test, tmodel, target, features, \"test\")\n",
        "    all_pred = pd.concat([train_pred, test_pred])\n",
        "\n",
        "    # save all predictions to model file\n",
        "    path2save = os.path.join(tmodel.path, \"predictions.csv\")\n",
        "    all_pred.to_csv(path2save, index=True)\n",
        "    preds_dic[model_type] = all_pred\n",
        "\n",
        "plot_rmse_over_experiments(preds_dic, exp_comparison_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_rmse_over_time(preds_dic, ls_model_types, exp_comparison_output)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate best models: train vs. val - performance and config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_all, best_configs = aggregate_best_models_metrics_and_configs(result_dic)\n",
        "\n",
        "plot_best_models_comparison(metrics_all, exp_comparison_output)\n",
        "\n",
        "# save best_configs to file\n",
        "best_configs.to_csv(os.path.join(exp_comparison_output, \"config.csv\"), index=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate one model over iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_model_training_over_iterations(\"xgb\", result_dic, labels=[\"data_transform\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Postrun evaluation over all experiments performed\n",
        "experiment > trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "overall_comparison_output = os.path.join(\"models\", \"compare_all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# find best trial over all experiments for each model type\n",
        "best_trials_overall = {}\n",
        "for model in ls_model_types:\n",
        "    # read all ExperimentAnalysis objects from this directory\n",
        "    experiment_dir = f\"best_models/*/{model}\"\n",
        "    analyses_ls = get_all_exp_analyses(experiment_dir)\n",
        "\n",
        "    # identify best trial from all analyses of this model type\n",
        "    best_trials_overall[model] = best_trial_name(analyses_ls, \"rmse_val\", mode=\"min\")\n",
        "\n",
        "compare_trials(best_trials_overall, overall_comparison_output)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "raytune",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
